Automatically generated by Mendeley Desktop 1.19.6
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Khan2020,
author = {Khan, Qadeer},
file = {:home/patricia/tutorial/week7/presentation.pdf:pdf},
number = {December},
title = {{Learning For Self-Driving Cars and Intelligent Systems P1 : Model Pruning}},
year = {2020}
}
@article{He2018,
abstract = {Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4 × FLOPs reduction, we achieved 2.7{\%} better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53 × on the GPU (Titan Xp) and 1.95 × on an Android phone (Google Pixel 1), with negligible loss of accuracy.},
archivePrefix = {arXiv},
arxivId = {1802.03494},
author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li Jia and Han, Song},
doi = {10.1007/978-3-030-01234-2_48},
eprint = {1802.03494},
file = {:home/patricia/Downloads/model{\_}pruning11.pdf:pdf},
isbn = {9783030012335},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {AutoML,CNN acceleration,Mobile vision,Model compression,Reinforcement learning},
pages = {815--832},
title = {{AMC: AutoML for model compression and acceleration on mobile devices}},
volume = {11211 LNCS},
year = {2018}
}
@article{Liu2018,
abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned “important” weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited “important” weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the “Lottery Ticket Hypothesis” (Frankle {\&} Carbin, 2019), and find that with optimal learning rate, the “winning ticket” initialization as used in Frankle {\&} Carbin (2019) does not bring improvement over random initialization.},
archivePrefix = {arXiv},
arxivId = {1810.05270},
author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
eprint = {1810.05270},
file = {:home/patricia/Downloads/model{\_}pruning5.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--21},
title = {{Rethinking the Value of Network Pruning}},
year = {2018}
}
@article{Frankle2018,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90{\%}, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that—when trained in isolation—reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20{\%} of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, Jonathan and Carbin, Michael},
eprint = {1803.03635},
file = {:home/patricia/Downloads/model{\_}pruning7.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--42},
title = {{The lottery ticket hypothesis: Finding sparse, trainable neural networks}},
year = {2018}
}
@article{Luo2017,
abstract = {We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31 x FLOPs reduction and 16.63× compression on VGG-16, with only 0.52{\%} top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1{\%} top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.},
archivePrefix = {arXiv},
arxivId = {1707.06342},
author = {Luo, Jian Hao and Wu, Jianxin and Lin, Weiyao},
doi = {10.1109/ICCV.2017.541},
eprint = {1707.06342},
file = {:home/patricia/Downloads/model{\_}pruning14.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5068--5076},
title = {{ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression}},
volume = {2017-Octob},
year = {2017}
}
@article{Wang2019,
abstract = {Network pruning is an important research field aiming at reducing computational costs of neural networks. Conventional approaches follow a fixed paradigm which first trains a large and redundant network, and then determines which units (e.g., channels) are less important and thus can be removed. In this work, we find that pre-training an over-parameterized model is not necessary for obtaining the target pruned structure. In fact, a fully-trained over-parameterized model will reduce the search space for the pruned structure. We empirically show that more diverse pruned structures can be directly pruned from randomly initialized weights, including potential models with better performance. Therefore, we propose a novel network pruning pipeline which allows pruning from scratch. In the experiments for compressing classification models on CIFAR10 and ImageNet datasets, our approach not only greatly reduces the pre-training burden of traditional pruning methods, but also achieves similar or even higher accuracy under the same computation budgets. Our results facilitate the community to rethink the effectiveness of existing techniques used for network pruning.},
archivePrefix = {arXiv},
arxivId = {1909.12579},
author = {Wang, Yulong and Zhang, Xiaolu and Xie, Lingxi and Zhou, Jun and Su, Hang and Zhang, Bo and Hu, Xiaolin},
doi = {10.1609/aaai.v34i07.6910},
eprint = {1909.12579},
file = {:home/patricia/Downloads/model{\_}pruning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
keywords = {Vision},
title = {{Pruning from scratch}},
year = {2019}
}
@article{Li2017,
abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34{\%} and ResNet-110 by up to 38{\%} on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
archivePrefix = {arXiv},
arxivId = {1608.08710},
author = {Li, Hao and Samet, Hanan and Kadav, Asim and Durdanovic, Igor and Graf, Hans Peter},
eprint = {1608.08710},
file = {:home/patricia/Downloads/model{\_}pruning13.pdf:pdf},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
number = {2016},
pages = {1--13},
title = {{Pruning filters for efficient convnets}},
year = {2017}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:home/patricia/Downloads/vgg16.pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Liu2018a,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
eprint = {1806.09055},
file = {:home/patricia/Downloads/model{\_}pruning12.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1--13},
title = {{Darts: Differentiable architecture search}},
year = {2018}
}
@article{Lin2020,
abstract = {Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate our method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models. Moreover, their performance surpasses that of models generated by all previously proposed pruning schemes.},
archivePrefix = {arXiv},
arxivId = {2006.07253},
author = {Lin, Tao and Barba, Luis and Stich, Sebastian U. and Dmitriev, Daniil and Jaggi, Martin},
eprint = {2006.07253},
file = {:home/patricia/Downloads/model{\_}pruning10.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Dynamic Model Pruning With Feedback}},
year = {2020}
}
@article{He2017,
abstract = {In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5× speed-up along with only 0.3{\%} increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4{\%}, 1.0{\%} accuracy loss under 2× speedup respectively, which is significant. Code has been made publicly available1.},
author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
file = {:home/patricia/Downloads/model{\_}pruning8.pdf:pdf},
issn = {23318422},
journal = {arXiv},
pages = {1389--1397},
title = {{Channel pruning for accelerating very deep neural networks}},
year = {2017}
}
@article{Guo2020,
abstract = {In this work, we propose a unified model compression framework called Multi-Dimensional Pruning (MDP) to simultaneously compress the convolutional neural networks (CNNs) on multiple dimensions. In contrast to the existing model compression methods that only aim to reduce the redundancy along either the spatial/spatial-temporal dimension (e.g., spatial dimension for 2D CNNs, spatial and temporal dimensions for 3D CNNs) or the channel dimension, our newly proposed approach can simultaneously reduce the spatial/spatial-temporal and the channel redundancies for CNNs. Specifically, in order to reduce the redundancy along the spatial/spatial-temporal dimension, we downsample the input tensor of a convolutional layer, in which the scaling factor for the downsampling operation is adaptively selected by our approach. After the convolution operation, the output tensor is upsampled to the original size to ensure the unchanged input size for the subsequent CNN layers. To reduce the channel-wise redundancy, we introduce a gate for each channel of the output tensor as its importance score, in which the gate value is automatically learned. The channels with small importance scores will be removed after the model compression process. Our comprehensive experiments on four benchmark datasets demonstrate that our MDP framework outperforms the existing methods when pruning both 2D CNNs and 3D CNNs.},
author = {Guo, Jinyang and Ouyang, Wanli and Xu, Dong},
doi = {10.1109/CVPR42600.2020.00158},
file = {:home/patricia/Downloads/model{\_}pruning4.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1505--1514},
title = {{Multi-Dimensional Pruning: A Unified Framework for Model Compression}},
volume = {1},
year = {2020}
}
@article{Zhu2017,
abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015a; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
archivePrefix = {arXiv},
arxivId = {1710.01878},
author = {Zhu, Michael H. and Gupta, Suyog},
eprint = {1710.01878},
file = {:home/patricia/Downloads/model{\_}pruning6.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{To prune, or not to prune: Exploring the efficacy of pruning for model compression}},
year = {2017}
}
@article{Tang2018,
abstract = {Nowadays, credit classification models are widely applied because they can help financial decision-makers to handle credit classification issues. Among them, artificial neural networks (ANNs) have been widely accepted as the convincing methods in the credit industry. In this paper, we propose a pruning neural network (PNN) and apply it to solve credit classification problem by adopting the well-known Australian and Japanese credit datasets. The model is inspired by synaptic nonlinearity of a dendritic tree in a biological neural model. And it is trained by an error back-propagation algorithm. The model is capable of realizing a neuronal pruning function by removing the superfluous synapses and useless dendrites and forms a tidy dendritic morphology at the end of learning. Furthermore, we utilize logic circuits (LCs) to simulate the dendritic structures successfully which makes PNN be implemented on the hardware effectively. The statistical results of our experiments have verified that PNN obtains superior performance in comparison with other classical algorithms in terms of accuracy and computational efficiency.},
author = {Tang, Yajiao and Ji, Junkai and Gao, Shangce and Dai, Hongwei and Yu, Yang and Todo, Yuki},
doi = {10.1155/2018/9390410},
file = {:home/patricia/Downloads/model{\_}pruning3.pdf:pdf},
issn = {16875273},
journal = {Computational intelligence and neuroscience},
pages = {9390410},
pmid = {29606961},
title = {{A Pruning Neural Network Model in Credit Classification Analysis}},
volume = {2018},
year = {2018}
}
@article{Liu2017,
abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20× reduction in model size and a 5× reduction in computing operations.},
archivePrefix = {arXiv},
arxivId = {1708.06519},
author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
doi = {10.1109/ICCV.2017.298},
eprint = {1708.06519},
file = {:home/patricia/Downloads/model{\_}pruning1.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2755--2763},
title = {{Learning Efficient Convolutional Networks through Network Slimming}},
volume = {2017-Octob},
year = {2017}
}
@misc{Mart2020,
	author = {Kim Martineau},
	title = {{A foolproof way to shrink deep learning models}},
	howpublished = {\url{https://news.mit.edu/2020/foolproof-way-shrink-deep-learning-models-0430}},
	year = {2020},
	note = {[Online; accessed 20-March-2021]}
}
